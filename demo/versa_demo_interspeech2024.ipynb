{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VERSA Colab Demonstration\n",
        "\n",
        "In this demonstration, we will show you some examples of VERSA (Versatile Speech and Audio) evaluation toolkit.\n",
        "\n",
        "Main reference:\n",
        "- [VERSA repository]()\n",
        "- [ESPnet repository]()\n",
        "\n",
        "Author:\n",
        "- Jiatong Shi ([email](jiatongs@andrew.cmu.edu))\n",
        "\n"
      ],
      "metadata": {
        "id": "kph7hap8FHtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Motivation:\n",
        "\n",
        "Evaluating the quality of generated speech and audio presents numerous challenges:\n",
        "- Collecting subjective evaluations is not easy.\n",
        "- Implementing objective evaluations is equally challenging.\n",
        "- There is a growing need for multi-domain, large-scale evaluations with unified speech, audio, and music modeling.\n",
        "\n",
        "Although new metrics are emerging in the field of speech and audio evaluation, researchers still face significant obstacles in accessing a broad range of evaluation metrics.\n",
        "\n",
        "VERSA aims to provide a general interface for speech and audio evaluation, offering a collection of both conventional and recent automatic quality evaluation metrics. While it can function independently, we also offer seamless integration with existing ESPnet tasks."
      ],
      "metadata": {
        "id": "LuWEKv2cJe_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contents\n",
        "\n",
        "1. VERSA installation\n",
        "\n",
        "2. VERSA Examples with API\n",
        "  \n",
        "  2.1 VERSA base evaluation\n",
        "\n",
        "  2.2 VERSA speaker evaluation\n",
        "\n",
        "  2.3 VERSA singing MOS evaluation\n",
        "\n",
        "3. VERSA Realtime Demonstration\n",
        "\n",
        "4. Contact"
      ],
      "metadata": {
        "id": "idR_ePEbIa4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 tensorflow==2.12.0 jax==0.4.9 jaxlib==0.4.9 ml_dtypes==0.2.0 transformers\n",
        "# IMPORTANT NOTE: due to the recent default change in colab, we need to restart the session to make the numpy(1.23.5) work as expected\n",
        "# Go to `Runtime` and select `Restart Session`"
      ],
      "metadata": {
        "id": "WLTCi9R1-zaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. VERSA Installation\n",
        "\n",
        "The VERSA can be easily installed with `pip` easily. By default, most of the supported metrics are included (check the default supported metrics in [Metric List](https://github.com/shinjiwlab/versa?tab=readme-ov-file#list-of-metrics)), while some other metrics are left as optional.\n",
        "\n",
        "\n",
        "For detailed instructions on installing specific metrics, please refer to the installation scripts/guides available at https://github.com/shinjiwlab/versa/tree/main/tools"
      ],
      "metadata": {
        "id": "5reu_XgrHpQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjglfpAQIWVc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# It takes 3-5 minutes.\n",
        "!git clone --depth 5 https://github.com/shinjiwlab/versa.git\n",
        "%cd /content/versa\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. VERSA Examples with API\n",
        "\n",
        "In this section, we demonstrate several examples using the VERSA scorer API, which serves as the primary interface for VERSA-supported metrics.\n",
        "\n",
        "The first step involves downloading some prepared audio samples along with example configuration files."
      ],
      "metadata": {
        "id": "Z4o9n_qdIlH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ftshijt/versa_demo_egs.git"
      ],
      "metadata": {
        "id": "O7yp4lVkI50W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 VERSA base evaluation\n",
        "\n",
        "In this section, we showcase some basic evaluations supported by VERSA.\n",
        "\n",
        "First, let's review the configuration used for the evaluation. We offer two versions: a CPU version and a GPU version, indicating which metrics require only a CPU and which can be accelerated with a GPU."
      ],
      "metadata": {
        "id": "LEhagT48WYWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CPU metrics\")\n",
        "\n",
        "!cat /content/versa/versa_demo_egs/configs/codec_16k_cpu.yaml\n",
        "\n",
        "print(\"***\" * 20)\n",
        "\n",
        "\n",
        "print(\"GPU metrics\")\n",
        "!cat /content/versa/versa_demo_egs/configs/codec_16k_gpu.yaml"
      ],
      "metadata": {
        "id": "wR4hxuNnWuCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can then use the configuration to evaluate the speech signals. In this demo, we have prepared a few samples from the LibriSpeech development set (you can also try a real-time demo with customized audio later!)."
      ],
      "metadata": {
        "id": "j8LJrg5OmkP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Listening\n",
        "import soundfile\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "print(\"Ground Truth Audio (22050Hz) for Testing\")\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/normal_speech/codec/gt/1.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/normal_speech/codec/gt/2.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/normal_speech/codec/gt/3.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "\n",
        "print(\"Target Audio (16000Hz) for Testing (Resynthesized with Encodec codec)\")\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/normal_speech/codec/encodec/1.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/normal_speech/codec/encodec/2.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/normal_speech/codec/encodec/3.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Us8WGftKnh8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scoring is simple and can be done with a single command.\n",
        "\n",
        "- Note that audio files with different sampling rates will be automatically handled.\n",
        "- We also support multi-processing with a job scheduling system to speed up the process. For more details, please visit [this link](https://github.com/shinjiwlab/versa?tab=readme-ov-file#usage).\n",
        "- A reference signal is optional for scoring, but its absence will result in some metrics being skipped if they require it.\n",
        "- We accept either a folder of wave or Kaldi-style `wav.scp` as input."
      ],
      "metadata": {
        "id": "zDyVHJR6omZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python versa/bin/scorer.py --score_config /content/versa/versa_demo_egs/configs/codec_16k_cpu.yaml \\\n",
        "    --gt /content/versa/versa_demo_egs/examples/normal_speech/codec/gt \\\n",
        "    --pred /content/versa/versa_demo_egs/examples/normal_speech/codec/encodec \\\n",
        "    --io dir \\\n",
        "    --output_file codec_base_cpu.json"
      ],
      "metadata": {
        "id": "0MmDOHaEnQc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The utterance-level results will be stored in the codec_base_cpu.json file, and the aggregated results can be obtained using a provided script."
      ],
      "metadata": {
        "id": "_vzvklE-plOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat codec_base_cpu.json\n",
        "\n",
        "!python /content/versa/scripts/show_result.py codec_base_cpu.json"
      ],
      "metadata": {
        "id": "hvIk4Z5AqYl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By setting use_gpu=True, you can easily enable GPU acceleration for metrics that support it."
      ],
      "metadata": {
        "id": "JcOBsC3Kq_wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python versa/bin/scorer.py --score_config /content/versa/versa_demo_egs/configs/codec_16k_gpu.yaml \\\n",
        "    --gt /content/versa/versa_demo_egs/examples/normal_speech/codec/gt \\\n",
        "    --pred /content/versa/versa_demo_egs/examples/normal_speech/codec/encodec \\\n",
        "    --io dir \\\n",
        "    --use_gpu true \\\n",
        "    --output_file codec_base_gpu.json\n",
        "\n",
        "! cat codec_base_gpu.json\n",
        "\n",
        "!python /content/versa/scripts/show_result.py codec_base_gpu.json"
      ],
      "metadata": {
        "id": "dcxAEtXorKsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 VERSA Speaker evaluation\n",
        "\n",
        "The reference signal can include samples from the same speakers, enabling speaker embedding analysis. VERSA is proud to integrate ESPnet-SPK, supporting more than 10 speaker embedding models!\n",
        "\n",
        "- Reference:\n",
        "  - [ESPnet-SPK](https://arxiv.org/abs/2401.17230)\n",
        "  - [Available Speaker Embedding](https://huggingface.co/models?other=speaker-recognition&sort=trending&search=espnet)\n",
        "\n",
        "For test purposes,  we have prepared three folders (with data also from LibriSpeech):\n",
        "- spk1: target speaker\n",
        "- spk1-other: other speech from the same target speaker\n",
        "- spk2: another speaker\n"
      ],
      "metadata": {
        "id": "l4Mtcz74WuZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Listening\n",
        "import soundfile\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "print(\"Target Speaker (22050Hz) for Testing\")\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/spk/spk1/1.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/spk/spk1/2.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/spk/spk1/3.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "\n",
        "print(\"Other Speech from Target Speaker (22050Hz) for Testing\")\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/spk/spk1-other/1.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/spk/spk1-other/2.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/spk/spk1-other/3.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "\n",
        "print(\"Other Speaker (22050Hz) for Testing\")\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/spk/spk2/1.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/spk/spk2/2.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/spk/spk2/3.wav\")\n",
        "display(Audio(audio, rate=sr))"
      ],
      "metadata": {
        "id": "19fk34IFWxkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if the same speaker can be identified\n",
        "! python versa/bin/scorer.py --score_config /content/versa/versa_demo_egs/configs/codec_16k_spk.yaml \\\n",
        "    --gt /content/versa/versa_demo_egs/examples/spk/spk1 \\\n",
        "    --pred /content/versa/versa_demo_egs/examples/spk/spk1-other \\\n",
        "    --io dir \\\n",
        "    --use_gpu true \\\n",
        "    --output_file spk_base.json\n",
        "\n",
        "! cat spk_base.json\n",
        "\n",
        "!python /content/versa/scripts/show_result.py spk_base.json"
      ],
      "metadata": {
        "id": "xcTwnzA6viaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if different speakers can be identified\n",
        "! python versa/bin/scorer.py --score_config /content/versa/versa_demo_egs/configs/codec_16k_spk.yaml \\\n",
        "    --gt /content/versa/versa_demo_egs/examples/spk/spk1 \\\n",
        "    --pred /content/versa/versa_demo_egs/examples/spk/spk2 \\\n",
        "    --io dir \\\n",
        "    --use_gpu true \\\n",
        "    --output_file spk_base_different.json\n",
        "\n",
        "! cat spk_base_different.json\n",
        "\n",
        "!python /content/versa/scripts/show_result.py spk_base_different.json"
      ],
      "metadata": {
        "id": "oKQqRxTswDnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 VERSA singing MOS evaluation\n",
        "\n",
        "In this subsection, we demonstrate the use of two types of Mean Opinion Score (MOS) evaluations applied to singing voice evaluation, which is distinct from normal speech.\n",
        "\n",
        "References\n",
        "  - [UTMOS](https://arxiv.org/abs/2204.02152)\n",
        "  - [UTMOS implementation](https://github.com/tarepan/SpeechMOS)\n",
        "  - [SingMOS](https://arxiv.org/abs/2406.10911)\n",
        "  - [SingMOS implementation](https://github.com/South-Twilight/SingMOS)"
      ],
      "metadata": {
        "id": "q7pRaLRtWyGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use singing samples from the ACE-KiSing paper. Three samples are compared: baseline, baseline with proposed corpora, and ground truth.\n",
        "\n",
        "Reference:\n",
        "- [ACE-Opencpop and ACE-KiSing](https://arxiv.org/pdf/2401.17619)\n",
        "- [KiSing](https://arxiv.org/abs/2205.04029)"
      ],
      "metadata": {
        "id": "pft3MMo8wj00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Listening\n",
        "import soundfile\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "print(\"Singing voice synthesis baseline\")\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/sing/visinger2-kising.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "print(\"Singing voice synthesis baseline with proposed corpora\")\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/sing/visinger2-ace-kising.wav\")\n",
        "display(Audio(audio, rate=sr))\n",
        "print(\"Singing voice synthesis ground truth\")\n",
        "audio, sr = soundfile.read(\"/content/versa/versa_demo_egs/examples/sing/ground_truth.wav\")\n",
        "display(Audio(audio, rate=sr))"
      ],
      "metadata": {
        "id": "cRWR7lq_W2Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scoring with the model\n",
        "! python versa/bin/scorer.py --score_config /content/versa/versa_demo_egs/configs/codec_16k_sing.yaml \\\n",
        "    --pred /content/versa/versa_demo_egs/examples/sing \\\n",
        "    --io dir \\\n",
        "    --output_file sing_base.json\n",
        "\n",
        "! cat sing_base.json"
      ],
      "metadata": {
        "id": "fOSGdsdmyM5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. VERSA Realtime Demonstration"
      ],
      "metadata": {
        "id": "oNowxTDqW22s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try some realtime demonstration by recording your own voice.\n",
        "\n",
        "First, please record your voice into the system. To achieve that, we first initialize a audio recording function."
      ],
      "metadata": {
        "id": "WdyGRDZ12Y2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit to https://colab.research.google.com/drive/1Z6VIRZ_sX314hyev3Gm5gBqvm1wQVo-a#scrollTo=9ol3xVNL6gy3\n",
        "\n",
        "!pip install ffmpeg-python\n",
        "\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "\n",
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };\n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr\n"
      ],
      "metadata": {
        "id": "We4CUtImW6Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's start recording. The recording will be start right after you execute the block. Press stop to save the recording."
      ],
      "metadata": {
        "id": "zmbxnsD32jGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio, sr = get_audio()\n",
        "\n",
        "# Resample\n",
        "import librosa\n",
        "test_audio = librosa.resample(np.array(audio, dtype=np.float32), orig_sr=sr, target_sr=16000)\n",
        "\n",
        "import soundfile\n",
        "import os\n",
        "os.makedirs(\"test_audio\", exist_ok=True)\n",
        "soundfile.write('test_audio/test.wav', test_audio, 16000, 'PCM_16')"
      ],
      "metadata": {
        "id": "oe0dOqRx2k7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scoring with the model\n",
        "! python versa/bin/scorer.py --score_config /content/versa/versa_demo_egs/configs/codec_16k_sing.yaml \\\n",
        "    --pred test_audio \\\n",
        "    --io dir \\\n",
        "    --output_file realtime_demo.json\n",
        "\n",
        "! cat realtime_demo.json"
      ],
      "metadata": {
        "id": "EuX3d__P3Q9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Contact\n",
        "\n",
        "VERSA is rapidly expanding, please directly submit issues to [our repo](https://github.com/shinjiwlab/versa.git) or contact Jiatong Shi (jiatongs@andrew.cmu.edu)/Shinji Watanabe (shinjiw@ieee.org) if you want to contribute/propose or request new metrics/report bug."
      ],
      "metadata": {
        "id": "YWhT58gsW6x-"
      }
    }
  ]
}